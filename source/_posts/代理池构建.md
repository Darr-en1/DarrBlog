---
title: 代理池构建
copyright: true
permalink: 1
top: 0
date: 2023-07-26 15:00:06
tags:
  - python
  - async
categories:
  - 爬虫
  - python
  - async
password:
---

爬虫和代理总是密不可分，但是不论是免费还是付费的代理，都不能保证它们每一个都是可用的。一个不可用的代理势必非常影响爬虫的工作效率。因此，在使用代理时，需要提前做一下筛选，剔除不可用的代理，保留下可用代理，所以搭建一个代理池非常必要 <!--more-->

该项目主要针对企业级代理池设计实现，基于 [https://github.com/Python3WebSpider/ProxyPool](https://github.com/Python3WebSpider/ProxyPool)，作者  [崔庆才](https://cuiqingcai.com/) ，是我非常喜欢的一个技术大拿，他所著作的《书籍python3网络爬虫》真的既详细又有深度，非常推荐。

该项目源码地址：[https://github.com/Darr-en1/ProxyPool](https://github.com/Darr-en1/ProxyPool)

**项目优势：**

- **项目对原有项目进行了重构，使用async异步编程，引入fastapi，整个服务的性能得到显著提升**
- **依赖管理使用poetry，可以管理直接依賴和间接依赖，可以区分多环境依赖  [Poetry 完全入門指南](https://blog.kyomind.tw/python-poetry)**
- **通过black、isort、mypy实现严格的代码规范，保证统一的代码风格，并通过pre-commit确保代码的质量和一致性**

## 爬虫代理类型

### 代理ip

 代理ip主要分为短效代理或独享代理，爬虫推荐使用短效代理，单次请求代理服务商获取多个携带过期时间的ip列表

 收费模式一般是两种 

- 每日使用的ip数量(但单个ip 通常都会重复使用，否则消耗过大）
- 每次获取的ip数量(服务商通常会设置获取ip时间间隔，因此也需要重复使用)

代理 ip 的每一个ip通常不存在 qps 的限制,或较大的qps

这种模式下通常手动维护HTTP代理池，并记录过期时间，然后定期刷新ip列表，最好选取时效性较短的方案。每个厂商都不能保证返回的IP100% 有效(通常90%以上，奈何爬虫需求量大)，因此最好设置一个探活机制，保证代理池中的IP都是有效的

### 隧道代理

隧道代理非常简单，隧道代理无须自己提取代理IP，将IP提取的事情交给隧道来做，程序只需要对接一个隧道代理的固定地址即可

收费模式一般是两种 

- 短效型隧道: 每个IP的都有实效性，到期后隧道将自动提取并使用另一个IP。同时也允许手动触发更换提取IP，但有提取间隔时间
- 动态转发型隧道：每一个请求一个随机IP。随机IP来自全局IP池

隧道代理都会设置单秒调用隧道的qps，通过不同的选配设置qps的大小

这种模式下业务通常不需要维护代理池



**动态转发型隧道是最适用的代理方式，但是也是最贵的，这里我们选用短效代理**



## 代理池架构

![image-20230727105131106](/images/代理池构建/image-20230727105131106.png)



代理池架构分为四个部分，获取模块、存储模块、检测模块、接口模块。

- 存储模块：使用Redis的zset，key存储代理ip，score存储代理的过期时间。
- 获取模块：定时调用代理供应商，将获取的代理传递给存储模块保存。
- 检测模块：定时从存储模块获取代理，检测代理的可用性，对不可用的代理进行删除。
- 接口模块：使用fastapi 对外提供接口服务，获取可用代理。



## 代理池的实现

项目代码结构如下

![image-20230727115215768](/代理池构建/image-20230727115215768-0429938.png)



### 存储模块

```python
class RedisClient:
    """
    redis connection client of proxypool
    """

    def __init__(
        self,
        host=REDIS_HOST,
        port=REDIS_PORT,
        password=REDIS_PASSWORD,
        db=REDIS_DB,
        connection_string=REDIS_CONNECTION_STRING,
        **kwargs,
    ):
        """
        init redis client
        :param host: redis host
        :param port: redis port
        :param password: redis password
        :param connection_string: redis connection_string
        """
        # if set connection_string, just use it
        if connection_string:
            pool = ConnectionPool.from_url(
                connection_string, decode_responses=True, **kwargs
            )

        else:
            pool = ConnectionPool(
                host=host,
                port=port,
                password=password,
                db=db,
                decode_responses=True,
                **kwargs,
            )
        self.db = Redis(connection_pool=pool)

    async def add(
        self, proxy: Proxy, deduction: int = DEDUCTION_EXPIRATION_TIME
    ) -> int:
        """
        add proxy and set it to init expire
        :param proxy: proxy, ip:port
        :param deduction: Deduction of expiration time
        :return:
            0 exist
            1 not exist
        """
        if not is_valid_proxy(f"{proxy.host}:{proxy.port}"):
            logger.info(f"invalid proxy {proxy}, throw it")
            return 0
        return await self.db.zadd(REDIS_KEY, {proxy.string(): proxy.expire - deduction})

    async def batch_add(
        self, proxy_list: List[Proxy], deduction: int = DEDUCTION_EXPIRATION_TIME
    ) -> int:
        """
        batch add proxy and set it to init expire
        :param proxy_list: List[Proxy], ip:port
        :param deduction: Deduction of expiration time
        :return:
            0 all exist
            n  n do not exist
        """
        valid_proxy_list = []
        for proxy in proxy_list:
            if not is_valid_proxy(f"{proxy.host}:{proxy.port}"):
                logger.info(f"invalid proxy {proxy}, throw it")
            valid_proxy_list.append(proxy)
        if valid_proxy_list:
            return await self.db.zadd(
                REDIS_KEY,
                {
                    proxy.string(): proxy.expire - deduction
                    for proxy in valid_proxy_list
                },
            )
        return 0

    async def random(self) -> Proxy:
        """
        get random proxy
        firstly try to get proxy with max score
        if not exists, try to get proxy by rank
        if not exists, raise error
        :return: proxy, like 8.8.8.8:8
        """
        # try to get proxy with max score
        proxies = await self.db.zrangebyscore(
            REDIS_KEY, int(time.time()), "+inf", withscores=True
        )
        if len(proxies):
            return convert_proxy(choice(proxies))
        raise PoolEmptyException

    async def delete(self, proxy: Proxy) -> int:
        """
        delete proxy, if small
        :param proxy: proxy
        :return: new score
        """
        logger.info(f"{proxy.string()} remove")
        return await self.db.zrem(REDIS_KEY, proxy.string())

    async def expired_delete(self) -> int:
        """
        remove expired proxy
        :return: new score
        """
        logger.info(f"remove expired proxy")
        return await self.db.zremrangebyscore(REDIS_KEY, "-inf", int(time.time()))

    async def exists(self, proxy: Proxy) -> bool:
        """
        if proxy exists
        :param proxy: proxy
        :return: if exists, bool
        """
        return not await self.db.zscore(REDIS_KEY, proxy.string()) is None

    async def count(self) -> int:
        """
        get count of proxies
        :return: count, int
        """
        return await self.db.zcount(REDIS_KEY, int(time.time()), "+inf")

    async def all(self) -> List[Proxy]:
        """
        get all proxies
        :return: list of proxies
        """
        data = await self.db.zrangebyscore(
            REDIS_KEY, int(time.time()), "+inf", withscores=True
        )
        return convert_proxies(data)
```

我们使用redis的zset做数据库存储，通过异步连接池进行通信。zset可以保证每一个代理ip都是不重复的，通过score记录代理ip的过期时间时间戳，zset会根据每一个元素的score对集合进行排序，数值小的排在前面，数值大的排在后面，这样就可以通过`ZRANGEBYSCORE current_time +inf`获取所有未过期的代理ip。

检测模块通过`ZRANGEBYSCORE`获取所有未过期的代理列表然后检测可用性，通过`ZREM proxy_ip`  对不可用的代理删除，`ZREMRANGEBYSCORE -inf current_time ` 并对所有过期的代理删除。



### 获取模块

#### BaseCrawler实现

```python
class BaseCrawler:
    urls: List[str] = []

    # reraise True 返回原本异常
    @retry(
        stop=stop_after_attempt(RETRIES),
        reraise=True,
        wait=wait_random(min=1, max=3),
        before=before_log(logger, logging.DEBUG),
    )
    async def fetch(self, url, **kwargs):
        headers = Headers(headers=True).generate()
        kwargs.setdefault("timeout", GET_TIMEOUT)
        kwargs.setdefault("verify_ssl", False)
        kwargs.setdefault("headers", headers)
        async with RetryClient(
            raise_for_status=RAISE_FOR_STATUS, retry_options=RETRY_OPTION
        ) as client:
            async with client.get(url=url, **kwargs) as response:
                if response.status == 200:
                    return await response.text()

    def start_urls(self):
        return self.urls

    def parse(self, html: str) -> List[Proxy]:
        raise NotImplementedError

    def process(self, html: str) -> List[Proxy]:
        """
        used for parse html
        """
        return self.parse(html)

    async def crawl(self, url):
        """
        crawl main method
        """
        try:
            logger.info(f"fetching {url}")
            html = await self.fetch(url)
            if not html:
                return []
            proxy_list = self.process(html)
            logger.info(f"fetched proxy {proxy_list} from {url}")
            return proxy_list
        except asyncio.exceptions.TimeoutError:
            logger.error(
                f"crawler {self} crawled proxy unsuccessfully, "
                "please check if target url is valid or network issue"
            )
            return []

    def __aiter__(self):
        self.crawl_cursor = 0
        return self

    async def __anext__(self) -> List[Proxy]:
        if len(self.urls) == 0:
            self.urls = self.start_urls()
        if len(self.urls) == self.crawl_cursor:
            raise StopAsyncIteration
        url = self.urls[self.crawl_cursor]
        self.crawl_cursor += 1
        return await self.crawl(url)
```

上面定义了 BaseCrawler 通过代理网站获取代理ip，实现ip获取，请求重试等基础功能。当需要对接代理供应商时，只需要继承 BaseCrawler 然后重写 parse 方法并定义要请求爬取的urls即可，当需要获取动态的urls时则重写 start_urls 即可。

如果该代理后续不使用了则可以申明类变量`ignore = True `即可

#### FatezeroCrawler实现

```python
BASE_URL = "http://proxylist.fatezero.org/proxy.list"


class FatezeroCrawler(BaseCrawler):
    """
    Fatezero crawler,http://proxylist.fatezero.org
    """

    urls = [BASE_URL]

    def parse(self, html):
        """
        parse html file to get proxies
        :return:
        """

        hosts_ports = html.split("\n")
        proxy_list = []
        for addr in hosts_ports:
            if addr:
                ip_address = json.loads(addr)
                host = ip_address["host"]
                port = ip_address["port"]
                proxy_list.append(
                    Proxy(host=host, port=port, expire=int(time.time()) + 60 * 3)
                )
        return proxy_list
```

FatezeroCrawler 的实现非常简单，只需要重写 parse 方法即可

#### Crawler加载

```python
classes = []
for loader, name, is_pkg in pkgutil.walk_packages(__path__):
    module = loader.find_module(name).load_module(name)  # type: ignore
    for class_name, value in inspect.getmembers(module):
        globals()[class_name] = value
        if (
            inspect.isclass(value)
            and issubclass(value, BaseCrawler)
            and value is not BaseCrawler
            and not getattr(value, "ignore", False)
        ):
            classes.append(value)
```

通过package读取实现Crawler的加载，加载的模块分为private 和 public，企业级应用对于效率有一定要求因此都会使用供应商的代理，在private中定义即可，如果使用免费代理则在public中定义，实现方式一直，实则简单的进行了物理隔离。

#### Getter实现

```python
class Getter:
    """
    getter of proxypool
    """

    def __init__(self):
        """
        init db and crawlers
        """
        self.redis = RedisClient()
        self.crawlers_cls = classes
        self.crawlers = [crawler_cls() for crawler_cls in self.crawlers_cls]

    async def is_full(self):
        """
        if proxypool if full
        return: bool
        """
        return await self.redis.count() >= PROXY_NUMBER_MAX

    @logger.catch
    async def run(self):
        """
        run crawlers to get proxy
        :return:
        """
        if await self.is_full():
            return
        for crawler in self.crawlers:
            logger.info(f"crawler {crawler} to get proxy")
            async for proxy_list in crawler:
                new_proxy_number = await self.redis.batch_add(proxy_list)
                logger.info(
                    f"batch_add {proxy_list=} number of new proxy: {new_proxy_number=}"
                )
```

通过执行Getter().run()实现代理的加载



### 检测模块

检测模块则获取代理池中为过期的代理进行请求检测，对无效的代理进行剔除，并对过期的代理一并删除

```python
class Tester:
    """
    tester for testing proxies in queue
    """

    def __init__(self):
        """
        init redis
        """
        self.redis = RedisClient()
        self.loop = asyncio.get_event_loop()

    async def test(self, proxy: Proxy, sem: asyncio.Semaphore) -> None:
        """
        test single proxy
        :param proxy: Proxy object
        :return:
        """
        async with sem:
            async with aiohttp.ClientSession(
                connector=aiohttp.TCPConnector(ssl=False)
            ) as session:
                try:
                    logger.debug(f"testing {proxy.string()}")
                    # if TEST_ANONYMOUS is True, make sure that
                    # the proxy has the effect of hiding the real IP
                    if TEST_ANONYMOUS:
                        url = "https://httpbin.org/ip"
                        async with session.get(url, timeout=TEST_TIMEOUT) as response:
                            resp_json = await response.json()
                            origin_ip = resp_json["origin"]
                        async with session.get(
                            url, proxy=f"http://{proxy.string()}", timeout=TEST_TIMEOUT
                        ) as response:
                            resp_json = await response.json()
                            anonymous_ip = resp_json["origin"]
                        assert origin_ip != anonymous_ip
                        assert proxy.host == anonymous_ip
                    async with session.get(
                        TEST_URL,
                        proxy=f"http://{proxy.string()}",
                        timeout=TEST_TIMEOUT,
                        allow_redirects=False,
                    ) as response:
                        if response.status in TEST_VALID_STATUS:
                            logger.debug(f"proxy {proxy.string()} is valid")
                        else:
                            await self.redis.delete(proxy)
                            logger.debug(f"proxy {proxy.string()} is invalid, delete")
                except EXCEPTIONS:
                    await self.redis.delete(proxy)
                    logger.debug(f"proxy {proxy.string()} is invalid, delete")

    @logger.catch
    async def run(self):
        """
        test main method
        :return:
        """
        # event loop of aiohttp
        logger.info("stating tester...")
        semaphore = asyncio.Semaphore(MAX_WORKERS)
        proxies = await self.redis.all()
        logger.debug(f"testing proxies , count {len(proxies)}")
        if proxies:
            tasks = [
                asyncio.create_task(self.test(proxy, semaphore)) for proxy in proxies
            ]
            await asyncio.wait(tasks)

        await self.redis.expired_delete()
```



### 接口模块

接口模块具备一下功能：

- 获取单一随机代理
- 获取代理池总代理数
- 获取所有代理

我们使用fastapi实现，非常简单

```python
app = FastAPI()

if IS_DEV:
    app.debug = True


def build_conn():
    redis = None

    def inner():
        nonlocal redis
        if not redis:
            redis = RedisClient()
        return redis

    return inner


get_conn = build_conn()


@app.get("/ping")
async def index():
    """
    ping pong
    :return:
    """
    return "pong"


@app.get("/random")
async def get_proxy():
    """
    get a random proxy
    :return: get a random proxy
    """
    conn = get_conn()
    proxy = await conn.random()
    return proxy.string()


@app.get("/all")
async def get_proxy_all():
    """
    get a random proxy
    :return: get a random proxy
    """
    conn = get_conn()
    proxies = await conn.all()
    return proxies


@app.get("/count")
async def get_count():
    """
    get the count of proxies
    :return: count, int
    """
    conn = get_conn()
    return int(await conn.count())
```

成功运行之后可以通过 [http://localhost:5555/random](http://localhost:5555/random) 获取一个随机可用代理。

访问 [http://0.0.0.0:5555/docs#/](http://0.0.0.0:5555/docs#/) 即查看API详情。

### 调度模块

调度模块通过click 命令行工具实现

```python
CONTEXT_SETTINGS = dict(help_option_names=["-h", "--help"])


@click.group(context_settings=CONTEXT_SETTINGS)
@click.version_option(version="1.0.0")
def cli():
    """
    enter
    """
    pass


@cli.command()
@click.option(
    "--cycle",
    default=CYCLE_TESTER,
    required=True,
    type=click.IntRange(10, 30),
    help="Tester 运行周期，即间隔多久运行一次测试",
    show_default=True,
)
def run_tester(cycle):
    """
    run tester
    测试代理可用性
    """
    if not ENABLE_TESTER:
        logger.info("tester not enabled, exit")
        return
    tester = Tester()

    async def inner():
        loop = 0
        while True:
            logger.debug(f"getter loop {loop} start...")
            await tester.run()
            loop += 1
            await asyncio.sleep(cycle)

    asyncio.run(inner())


@cli.command()
@click.option(
    "--cycle",
    default=CYCLE_GETTER,
    required=True,
    type=click.IntRange(30, 180),
    help="Getter 运行周期，即间隔多久运行一次代理获取",
    show_default=True,
)
def run_getter(cycle):
    """
    run getter
    获取代理
    """
    if not ENABLE_GETTER:
        logger.info("getter not enabled, exit")
        return
    getter = Getter()

    async def inner():
        loop = 0
        while True:
            logger.debug(f"getter loop {loop} start...")
            await getter.run()
            loop += 1
            await asyncio.sleep(cycle)

    asyncio.run(inner())


@cli.command()
def run_server():
    """
    run server for api
    启动代理服务器
    """
    if not ENABLE_SERVER:
        logger.info("server not enabled, exit")
        return
    if IS_PROD:
        uvicorn.run(
            "proxypool.processors.server:app",
            host=API_HOST,
            port=API_PORT,
            workers=WORKERS,
        )
    else:
        uvicorn.run(
            app="proxypool.processors.server:app",
            host=API_HOST,
            port=API_PORT,
            reload=APP_DEBUG,
        )
```

调度模块中共实现了三个模块的调度，分别是

- Getter
- Server
- Tester

通过`python run.py  -h` 查看执行命令

```shell
Usage: run.py [OPTIONS] COMMAND [ARGS]...

  enter

Options:
  --version   Show the version and exit.
  -h, --help  Show this message and exit.

Commands:
  run-getter  run getter 获取代理
  run-server  run server for api 启动代理服务器
  run-tester  run tester 测试代理可用性
```

执行相应 Commands 即可完成服务启动。



以上便是完整的代理池实现方案，代码在我的github仓库：[https://github.com/Darr-en1/ProxyPool](https://github.com/Darr-en1/ProxyPool)，欢迎大家一起交流讨论
